name: "CI on Unix"

on:
  schedule:
    - cron: '0 * * * *'  # Every hour
  workflow_dispatch:

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  check-and-sync:
    name: "Check PHP versions and sync"
    runs-on: ubuntu-latest
    outputs:
      build-matrix: ${{ steps.sync.outputs.matrix }}
      eol-versions: ${{ steps.sync.outputs.eol }}
      should-build: ${{ steps.sync.outputs.should-build }}
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
    steps:
      - name: "Checkout"
        uses: actions/checkout@v5

      - name: "Sync PHP versions"
        id: sync
        run: |
          # Download current metadata.json from R2 (or create empty if not exists)
          if aws s3 cp s3://${{ secrets.R2_BUCKET_NAME }}/metadata.json metadata.json --endpoint-url ${{ secrets.R2_ENDPOINT }} 2>/dev/null; then
            echo "Downloaded existing metadata.json"
          else
            echo '{"last_sync": "", "versions": {}}' > metadata.json
            echo "Created empty metadata.json"
          fi

          # Fetch PHP.watch API
          echo "Fetching PHP.watch API..."
          curl -fsSL https://php.watch/api/v1/versions > api_response.json

          # Run check versions script
          python3 .github/scripts/php_build_manager.py check-versions
          cat github_output.txt >> "$GITHUB_OUTPUT"

  # Build job (dynamic matrix from check-and-sync)
  build:
    name: "Build PHP ${{ matrix.php-version }} on ${{ matrix.os }}"
    runs-on: ${{ matrix.runs-on }}
    needs: [check-and-sync]
    if: needs.check-and-sync.outputs.should-build == 'true'
    timeout-minutes: 240
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.check-and-sync.outputs.build-matrix) }}
    steps:
      - name: "Checkout"
        uses: actions/checkout@v5

      - name: "Setup PHP"
        uses: shivammathur/setup-php@v2
        with:
          php-version: 8.4
          tools: pecl, composer
          extensions: curl, openssl, mbstring
          ini-values: memory_limit=-1
        env:
          phpts: nts

      # Cache downloaded source
      - id: cache-download
        uses: actions/cache@v4
        with:
          path: downloads
          key: php-dependencies-${{ matrix.php-version }}-${{ matrix.os }}

      - name: "Download sources"
        run: |
          # Define extensions to download and build
          EXTENSIONS="bcmath,bz2,calendar,ctype,curl,dba,dom,exif,ffi,fileinfo,filter,ftp,gd,gmp,iconv,igbinary,imagick,imap,intl,ldap,lz4,mbstring,mongodb,mysqli,mysqlnd,opcache,openssl,pcntl,pdo,pdo_mysql,pdo_pgsql,pdo_sqlite,pdo_sqlsrv,pgsql,phar,posix,readline,redis,session,shmop,simplexml,soap,sockets,sodium,sqlite3,sqlsrv,sysvmsg,sysvsem,sysvshm,tokenizer,xml,xmlreader,xmlwriter,xsl,zip,zlib"

          # Define build commands for macOS aarch64
          DOWN_CMD="curl -fsSL -o spc https://dl.static-php.dev/static-php-cli/spc-bin/nightly/spc-macos-aarch64 && chmod +x spc && ./spc doctor --auto-fix && ./spc download"

          DOWNLOAD_CMD="$DOWN_CMD --with-php=${{ matrix.php-version }} --for-extensions=\"$EXTENSIONS\" --ignore-cache-sources=php-src --prefer-pre-built"
          eval "$DOWNLOAD_CMD"

          # Export extensions for next step
          echo "EXTENSIONS=$EXTENSIONS" >> $GITHUB_ENV

      - name: "Build PHP"
        run: |
          ./spc build $EXTENSIONS --build-cli --build-fpm -P .github/scripts/patch_fadogen_ini_scan.php

      # Create unified archive and upload to R2
      - name: "Create and Upload PHP archive to R2"
        run: |
          # Create tar.xz archive containing both CLI and FPM
          python3 .github/scripts/php_build_manager.py create-archive \
            --php-version ${{ matrix.php-version }} \
            --os ${{ matrix.os }}

          # Load archive info
          source archive_info.txt

          # Upload archive to R2
          aws s3 cp "$ARCHIVE_NAME" \
            s3://${{ secrets.R2_BUCKET_NAME }}/"$ARCHIVE_NAME" \
            --endpoint-url ${{ secrets.R2_ENDPOINT }}

      # Save checksum with unique filename
      - name: "Save checksum"
        run: |
          # Load archive info
          source archive_info.txt

          # Create checksum file with UNIQUE name per job
          echo "${{ matrix.php-version }},${{ matrix.os }},$ARCHIVE_SHA512" > checksums-${{ matrix.php-version }}-${{ matrix.os }}.txt

      # Upload checksum artifact (one per version-os)
      - name: "Upload checksums"
        uses: actions/upload-artifact@v4
        with:
          name: checksum-${{ matrix.php-version }}-${{ matrix.os }}
          path: checksums-${{ matrix.php-version }}-${{ matrix.os }}.txt
          retention-days: 1

  update-metadata:
    name: "Update central metadata.json"
    runs-on: ubuntu-latest
    needs: [check-and-sync, build]
    if: needs.build.result == 'success'
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
    steps:
      - name: "Checkout"
        uses: actions/checkout@v5

      # Download all checksum artifacts
      - name: "Download checksums"
        uses: actions/download-artifact@v5
        with:
          pattern: checksum-*
          merge-multiple: true

      - name: "Update metadata.json"
        run: |
          # Download current metadata.json from R2 (or create empty if not exists)
          if aws s3 cp s3://${{ secrets.R2_BUCKET_NAME }}/metadata.json metadata.json --endpoint-url ${{ secrets.R2_ENDPOINT }} 2>/dev/null; then
            echo "Downloaded existing metadata.json"
          else
            echo '{"last_sync": "", "versions": {}}' > metadata.json
            echo "Created empty metadata.json"
          fi

          BUILD_MATRIX='${{ needs.check-and-sync.outputs.build-matrix }}'

          # Consolidate all downloaded checksum files
          ALL_CHECKSUMS=""
          if ls checksums-*.txt 2>/dev/null; then
            ALL_CHECKSUMS=$(cat checksums-*.txt 2>/dev/null | grep -v "^$")
          fi

          # Update metadata with build results
          python3 .github/scripts/php_build_manager.py update-metadata \
            --build-matrix "$BUILD_MATRIX" \
            --archive-checksums "$ALL_CHECKSUMS"

          # Upload updated metadata.json to R2
          aws s3 cp metadata.json s3://${{ secrets.R2_BUCKET_NAME }}/metadata.json --endpoint-url ${{ secrets.R2_ENDPOINT }}
          echo "Uploaded metadata.json to R2"

  cleanup-eol:
    name: "Cleanup EOL versions"
    runs-on: ubuntu-latest
    needs: [check-and-sync, update-metadata]
    if: needs.check-and-sync.outputs.eol-versions != '[]'
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
    steps:
      - name: "Checkout"
        uses: actions/checkout@v5

      - name: "Cleanup EOL versions"
        run: |
          EOL_VERSIONS='${{ needs.check-and-sync.outputs.eol-versions }}'
          echo "EOL versions to cleanup: $EOL_VERSIONS"

          # Define all OS targets
          ALL_OS="macos-aarch64"

          # Parse EOL versions from JSON array
          echo "$EOL_VERSIONS" | jq -r '.[]' | while read -r version; do
            echo "Cleaning up PHP $version..."

            for os in $ALL_OS; do
              echo "  Removing files for $version on $os"

              # Remove archive
              aws s3 rm s3://${{ secrets.R2_BUCKET_NAME }}/php-$version-$os.tar.xz \
                --endpoint-url ${{ secrets.R2_ENDPOINT }} 2>/dev/null || echo "    php-$version-$os.tar.xz not found"
            done
          done

          # Remove EOL versions from metadata.json
          if aws s3 cp s3://${{ secrets.R2_BUCKET_NAME }}/metadata.json metadata.json --endpoint-url ${{ secrets.R2_ENDPOINT }} 2>/dev/null; then
            python3 .github/scripts/php_build_manager.py cleanup-eol \
              --eol-versions '${{ needs.check-and-sync.outputs.eol-versions }}'
            aws s3 cp metadata.json s3://${{ secrets.R2_BUCKET_NAME }}/metadata.json --endpoint-url ${{ secrets.R2_ENDPOINT }}
            echo "Updated metadata.json after EOL cleanup"
          fi

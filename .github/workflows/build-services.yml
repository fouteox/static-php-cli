name: "CI Services - Build Database Binaries"

on:
  schedule:
    - cron: '0 8 * * *'  # Daily at 8 AM UTC
  workflow_dispatch:

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  check-and-sync-services:
    name: "Check service versions and sync"
    runs-on: ubuntu-latest
    outputs:
      build-matrix: ${{ steps.sync.outputs.matrix }}
      should-build: ${{ steps.sync.outputs.should-build }}
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
    steps:
      - name: "Checkout"
        uses: actions/checkout@v5

      - name: "Sync service versions"
        id: sync
        run: |
          # Download current metadata-services.json from R2 (or create empty if not exists)
          if aws s3 cp s3://${{ secrets.R2_BUCKET_NAME }}/metadata-services.json metadata-services.json --endpoint-url ${{ secrets.R2_ENDPOINT }} 2>/dev/null; then
            echo "Downloaded existing metadata-services.json"
          else
            echo '{"last_sync": "", "mariadb": {}, "mysql": {}, "postgresql": {}, "redis": {}}' > metadata-services.json
            echo "Created empty metadata-services.json"
          fi

          # Run check versions script
          python3 .github/scripts/services_build_manager.py check-versions
          cat github_output.txt >> "$GITHUB_OUTPUT"

  # Build job (dynamic matrix from check-and-sync-services)
  build-services:
    name: "Build ${{ matrix.service }} ${{ matrix.full-version }} on ${{ matrix.os }}"
    runs-on: ${{ matrix.runs-on }}
    needs: [check-and-sync-services]
    if: needs.check-and-sync-services.outputs.should-build == 'true'
    timeout-minutes: 240
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
      FULL_VERSION: ${{ matrix.full-version }}
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.check-and-sync-services.outputs.build-matrix) }}
    steps:
      - name: "Checkout"
        uses: actions/checkout@v5

      - name: "Setup dependencies"
        run: |
          # Install dependencies based on service
          case "${{ matrix.service }}" in
            "mariadb")
              brew install bison || true
              ;;
            "mysql")
              brew install bison boost openssl ncurses || true
              ;;
            "postgresql")
              brew install pkgconf readline openssl icu4c || true
              ;;
            "redis")
              brew install coreutils make openssl llvm@18 gnu-sed automake libtool wget rust || true
              ;;
          esac

      # Generate unique timestamp - SINGLE source of truth
      - name: "Generate Build Timestamp"
        id: timestamp
        run: |
          TIMESTAMP=$(date -u +"%Y%m%d%H%M%S")
          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "Generated timestamp: $TIMESTAMP"

      - name: "Build ${{ matrix.service }}"
        run: |
          echo "[INFO] Building ${{ matrix.service }} version ${{ matrix.full-version }}"
          echo "[INFO] FULL_VERSION environment variable: $FULL_VERSION"

          # Run the appropriate build script
          .github/scripts/${{ matrix.service }}-build.sh ${{ matrix.major-version }}

      # Create unified archive and upload to R2 - ATOMIC
      - name: "Create and Upload ${{ matrix.service }} archive to R2"
        run: |
          # STRICT: Create archive with REQUIRED timestamp
          python3 .github/scripts/services_build_manager.py create-archive \
            --service ${{ matrix.service }} \
            --full-version ${{ matrix.full-version }} \
            --os ${{ matrix.os }} \
            --timestamp ${{ steps.timestamp.outputs.timestamp }}

          # Load archive info
          source archive_info.txt
          echo "Archive created: $ARCHIVE_NAME with SHA512: $ARCHIVE_SHA512"

          # ATOMIC: Upload archive with unique filename - no collision possible
          aws s3 cp "$ARCHIVE_NAME" \
            s3://${{ secrets.R2_BUCKET_NAME }}/"$ARCHIVE_NAME" \
            --endpoint-url ${{ secrets.R2_ENDPOINT }}

      # Save checksum with filename - STRICT format
      - name: "Save checksum"
        run: |
          # Load archive info
          source archive_info.txt

          # STRICT: Create checksum file with ALL 4 required fields (full_version,os,sha512,filename)
          echo "${{ matrix.full-version }},${{ matrix.os }},$ARCHIVE_SHA512,$ARCHIVE_NAME" > checksums-${{ matrix.service }}-${{ matrix.full-version }}-${{ matrix.os }}.txt
          echo "Checksum saved: ${{ matrix.full-version }},${{ matrix.os }},$ARCHIVE_SHA512,$ARCHIVE_NAME"

      # Upload checksum artifact (one per service-version-os)
      - name: "Upload checksums"
        uses: actions/upload-artifact@v4
        with:
          name: checksum-${{ matrix.service }}-${{ matrix.full-version }}-${{ matrix.os }}
          path: checksums-${{ matrix.service }}-${{ matrix.full-version }}-${{ matrix.os }}.txt
          retention-days: 1

  update-metadata-services:
    name: "Update central metadata-services.json"
    runs-on: ubuntu-latest
    needs: [check-and-sync-services, build-services]
    if: needs.build-services.result == 'success'
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: us-east-1
    steps:
      - name: "Checkout"
        uses: actions/checkout@v5

      # Download all checksum artifacts
      - name: "Download checksums"
        uses: actions/download-artifact@v5
        with:
          pattern: checksum-*
          merge-multiple: true

      - name: "Update metadata-services.json"
        run: |
          # Download current metadata-services.json from R2 (or create empty if not exists)
          if aws s3 cp s3://${{ secrets.R2_BUCKET_NAME }}/metadata-services.json metadata-services.json --endpoint-url ${{ secrets.R2_ENDPOINT }} 2>/dev/null; then
            echo "Downloaded existing metadata-services.json"
          else
            echo '{"last_sync": "", "mariadb": {}, "mysql": {}, "postgresql": {}, "redis": {}}' > metadata-services.json
            echo "Created empty metadata-services.json"
          fi

          BUILD_MATRIX='${{ needs.check-and-sync-services.outputs.build-matrix }}'

          # Consolidate all downloaded checksum files
          ALL_CHECKSUMS=""
          if ls checksums-*.txt 2>/dev/null; then
            ALL_CHECKSUMS=$(cat checksums-*.txt 2>/dev/null | grep -v "^$")
          fi

          # Update metadata with build results
          python3 .github/scripts/services_build_manager.py update-metadata \
            --build-matrix "$BUILD_MATRIX" \
            --archive-checksums "$ALL_CHECKSUMS"

          # Upload updated metadata-services.json to R2
          aws s3 cp metadata-services.json s3://${{ secrets.R2_BUCKET_NAME }}/metadata-services.json --endpoint-url ${{ secrets.R2_ENDPOINT }}
          echo "Uploaded metadata-services.json to R2"

      # YAGNI: Simple cleanup - keep only last 3 versions per service major version
      - name: "Cleanup old service binaries"
        run: |
          echo "Starting cleanup of old service binaries..."

          # Cleanup for each service in the build matrix
          BUILD_MATRIX='${{ needs.check-and-sync-services.outputs.build-matrix }}'
          echo "$BUILD_MATRIX" | jq -r '.include[] | "\(.service)-\(."major-version")"' | sort -u | while read service_major; do
            echo "Cleaning up old binaries for $service_major"

            # List all files for this service-major, sort by last modified, keep newest 3, delete the rest
            aws s3api list-objects-v2 \
              --bucket ${{ secrets.R2_BUCKET_NAME }} \
              --prefix "$service_major-" \
              --endpoint-url ${{ secrets.R2_ENDPOINT }} \
              --query 'sort_by(Contents, &LastModified)[:-3].Key' \
              --output text | \
            while read -r file_key; do
              if [ "$file_key" != "None" ] && [ -n "$file_key" ]; then
                echo "Deleting old binary: $file_key"
                aws s3 rm s3://${{ secrets.R2_BUCKET_NAME }}/"$file_key" \
                  --endpoint-url ${{ secrets.R2_ENDPOINT }} || echo "Failed to delete $file_key"
              fi
            done
          done

          echo "Cleanup completed"